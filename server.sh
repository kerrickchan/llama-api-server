python -m llama_cpp.server --model ./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --chat_format chatml
